{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineer Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment 1:\n",
    "\n",
    "Building ETL Pipelines for Multi-source Data Ingestion\n",
    "This assessment is designed to evaluate the skills and knowledge of a data engineer\n",
    "specializing in designing and building ETL pipelines for pulling data from various sources,\n",
    "including Facebook Ads, Google Ads, RDS, CleverTap, etc. The assessment will also cover\n",
    "the use of relevant tools such as Apache Airflow, Kubernetes, and other associated\n",
    "technologies.\n",
    "\n",
    "\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "- Accuracy and depth of explanations.\n",
    "- Clarity and coherence of architectural and technical designs.\n",
    "- Demonstrated understanding of Apache Airflow and Kubernetes.\n",
    "- Practicality of data transformation and error handling strategies.\n",
    "- Thoroughness of security measures and compliance considerations.\n",
    "- Creativity and effectiveness in performance optimization approaches.\n",
    "- Importance of documentation and collaboration in the provided context.\n",
    "- Real-world adaptability and problem-solving skills.\n",
    "Please note that this assessment is designed to gauge a candidate's expertise in data\n",
    "engineering, ETL pipeline design, and associated technologies. The candidate's ability to\n",
    "effectively communicate their solutions, strategies, and thought processes will also be\n",
    "evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 1: Data Source Understanding**\n",
    "\n",
    "1. Explain the differences between Facebook Ads, Google Ads, RDS (Relational Database\n",
    "Service), and CleverTap in terms of data structure, API access, and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 2: ETL Pipeline Design**\n",
    "\n",
    "2. Design a high-level ETL pipeline architecture for extracting data from Facebook Ads and\n",
    "Google Ads, transforming it, and loading it into an RDS database. Consider data extraction\n",
    "frequency, data transformations, error handling, and scalability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 3: Apache Airflow**\n",
    "\n",
    "3. What is Apache Airflow, and how does it facilitate ETL pipeline orchestration? Provide an\n",
    "example of an Airflow DAG (Directed Acyclic Graph) for scheduling and orchestrating the\n",
    "ETL process described in Section 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- Airflow is an open-source tool for orchestrating complex workflows.\n",
    "- It uses Directed Acyclic Graphs (DAGs) to define the sequence of tasks.\n",
    "\n",
    "**Example Airflow DAG:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'airflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mairflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DAG\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mairflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdummy_operator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DummyOperator\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mairflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython_operator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PythonOperator\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'airflow'"
     ]
    }
   ],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def extract_facebook_ads():\n",
    "    # Code to extract data from Facebook Ads API\n",
    "    pass\n",
    "\n",
    "def extract_google_ads():\n",
    "    # Code to extract data from Google Ads API\n",
    "    pass\n",
    "\n",
    "def transform_data():\n",
    "    # Code to transform extracted data\n",
    "    pass\n",
    "\n",
    "def load_to_rds():\n",
    "    # Code to load data into RDS (PostgreSQL)\n",
    "    pass\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2023, 1, 1),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG('etl_pipeline', default_args=default_args, schedule_interval='@daily')\n",
    "\n",
    "start = DummyOperator(task_id='start', dag=dag)\n",
    "extract_facebook = PythonOperator(task_id='extract_facebook_ads', python_callable=extract_facebook_ads, dag=dag)\n",
    "extract_google = PythonOperator(task_id='extract_google_ads', python_callable=extract_google_ads, dag=dag)\n",
    "transform = PythonOperator(task_id='transform_data', python_callable=transform_data, dag=dag)\n",
    "load = PythonOperator(task_id='load_to_rds', python_callable=load_to_rds, dag=dag)\n",
    "end = DummyOperator(task_id='end', dag=dag)\n",
    "\n",
    "start >> [extract_facebook, extract_google] >> transform >> load >> end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 4: Kubernetes Integration**\n",
    "\n",
    "4. Explain the role of Kubernetes in deploying and managing ETL pipelines. How can\n",
    "Kubernetes ensure scalability, fault tolerance, and resource optimization for ETL tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- Kubernetes is a container orchestration platform that automates the deployment, scaling, and management of containerized applications.\n",
    "- It ensures scalability by managing the resources and distributing the load across multiple nodes.\n",
    "- It provides fault tolerance by automatically restarting failed containers and managing container replicas.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- Scalability: Automatically scales ETL tasks based on workload.\n",
    "- Fault Tolerance: Ensures high availability by restarting failed containers.\n",
    "- Resource Optimization: Efficiently utilizes resources by distributing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 5: Data Transformation**\n",
    "\n",
    "5. Given a JSON data sample from Facebook Ads containing ad performance metrics, write\n",
    "a Python function to transform this data into a structured format suitable for storage in an\n",
    "AWS Redshift database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ad_id</th>\n",
       "      <th>impressions</th>\n",
       "      <th>clicks</th>\n",
       "      <th>spend</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ad1</td>\n",
       "      <td>1000</td>\n",
       "      <td>50</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2023-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ad2</td>\n",
       "      <td>2000</td>\n",
       "      <td>70</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2023-01-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ad_id  impressions  clicks  spend        date\n",
       "0   ad1         1000      50   10.0  2023-01-01\n",
       "1   ad2         2000      70   20.0  2023-01-02"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def transform_data(json_data):\n",
    "    # Load JSON data\n",
    "    data = json.loads(json_data)\n",
    "    \n",
    "    # Flatten JSON structure\n",
    "    ad_performance = []\n",
    "    for ad in data['ads']:\n",
    "        ad_performance.append({\n",
    "            'ad_id': ad['id'],\n",
    "            'impressions': ad['impressions'],\n",
    "            'clicks': ad['clicks'],\n",
    "            'spend': ad['spend'],\n",
    "            'date': ad['date'],\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(ad_performance)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example JSON data\n",
    "json_data = '''\n",
    "{\n",
    "    \"ads\": [\n",
    "        {\"id\": \"ad1\", \"impressions\": 1000, \"clicks\": 50, \"spend\": 10.0, \"date\": \"2023-01-01\"},\n",
    "        {\"id\": \"ad2\", \"impressions\": 2000, \"clicks\": 70, \"spend\": 20.0, \"date\": \"2023-01-02\"}\n",
    "    ]\n",
    "}\n",
    "'''\n",
    "\n",
    "# Transform JSON data\n",
    "df = transform_data(json_data)\n",
    "\n",
    "# Display data\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 6: Error Handling and Monitoring**\n",
    "\n",
    "6. Describe strategies for handling errors that may occur during the ETL process. How would\n",
    "you set up monitoring and alerting mechanisms to ensure the health and performance of the\n",
    "ETL pipelines?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error Handling:**\n",
    "- Implement retry logic for API calls.\n",
    "- Validate data before processing.\n",
    "- Log errors with detailed information for debugging.\n",
    "- Use fallback mechanisms for critical failures.\n",
    "\n",
    "**Monitoring and Alerting:**\n",
    "- Use monitoring tools to track ETL pipeline health.\n",
    "- Set up alerts for failures or performance issues.\n",
    "- Monitor resource usage (CPU, memory) to optimize performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 7: Security and Compliance**\n",
    "\n",
    "7. Data security is crucial when dealing with sensitive user information. Describe the\n",
    "measures you would take to ensure data security and compliance with relevant regulations\n",
    "while pulling and storing data from different sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Security Measures:**\n",
    "- Use encrypted connections (SSL/TLS) for data transfer.\n",
    "- Store sensitive data securely (e.g., using encryption).\n",
    "- Implement access controls and authentication mechanisms.\n",
    "- Regularly audit and review security policies.\n",
    "\n",
    "**Compliance:**\n",
    "- Ensure compliance with regulations (e.g., GDPR, HIPAA) by following best practices for data privacy.\n",
    "- Implement data anonymization and masking techniques.\n",
    "- Maintain detailed records of data processing activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 8: Performance Optimization**\n",
    "\n",
    "8. Discuss potential performance bottlenecks that might arise in the ETL process, particularly\n",
    "when dealing with large volumes of data. How would you optimize the ETL pipeline to\n",
    "ensure efficient data processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Potential Bottlenecks:**\n",
    "\n",
    "- API rate limits and latency.\n",
    "- Data transformation and processing time.\n",
    "- Database write performance.\n",
    "\n",
    "**Optimization Techniques:**\n",
    "\n",
    "- Use batch processing to reduce API calls.\n",
    "- Optimize data transformation logic (e.g., using vectorized operations).\n",
    "- Use bulk inserts for database loading.\n",
    "- Use caching and indexing for frequently accessed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 9: Documentation and Collaboration**\n",
    "\n",
    "9. How important is documentation in the context of ETL pipeline development? Describe the\n",
    "components you would include in documentation to ensure seamless collaboration with\n",
    "other team members and future maintainers of the pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Components of Documentation:**\n",
    "\n",
    "- High-level architecture diagrams.\n",
    "- Detailed descriptions of ETL processes.\n",
    "- Code comments and docstrings.\n",
    "- Instructions for setup and deployment.\n",
    "- Troubleshooting guides and common issues.\n",
    "\n",
    "**Importance:**\n",
    "\n",
    "- Facilitates collaboration and knowledge sharing.\n",
    "- Helps onboard new team members quickly.\n",
    "- Ensures maintainability and future scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Section 10: Real-world Scenario**\n",
    "\n",
    "10. You have been given a scenario where CleverTap's API structure has changed, affecting\n",
    "your ETL pipeline. Explain the steps you would take to adapt your existing pipeline to\n",
    "accommodate this change while minimizing disruptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps to Adapt:**\n",
    "- Review and understand the changes in the API structure.\n",
    "- Update the ETL pipeline code to handle new data formats.\n",
    "- Test the updated pipeline with sample data.\n",
    "- Deploy changes with minimal disruption (e.g., using feature flags).\n",
    "- Monitor the pipeline closely after deployment for any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment 2: \n",
    "\n",
    "Instructions: This assessment is designed to evaluate the technical knowledge and skills of\n",
    "a data engineer specialising in building ETL pipelines for extracting data from various\n",
    "sources such as Facebook Ads, Google Ads, RDS (Relational Database Service),\n",
    "CleverTap, etc. using tools like Apache Airflow, Kubernetes, and other relevant technologies.\n",
    "Please provide detailed explanations wherever required.\n",
    "\n",
    "**Part 1: Data Source Integration**\n",
    "\n",
    "1. Explain the steps you would take to extract data from Facebook Ads and Google Ads\n",
    "APIs. Highlight the authentication process and any specific considerations for handling API\n",
    "rate limits.\n",
    "2. How would you design an ETL process to pull data from a relational database (RDS) like\n",
    "MySQL or PostgreSQL? Discuss the factors you would consider to ensure efficient and\n",
    "reliable data extraction.\n",
    "3. CleverTap provides event-based user data. How would you approach extracting and\n",
    "transforming this event data into a usable format for further analysis? Outline the key\n",
    "components of your ETL pipeline.\n",
    "\n",
    "**Part 2: ETL Pipeline Development**\n",
    "\n",
    "4. What is Apache Airflow? Describe its role in building and managing ETL pipelines.\n",
    "Provide an example of an Airflow DAG (Directed Acyclic Graph) that orchestrates the ETL\n",
    "process you described in question 3.\n",
    "5. Kubernetes is a container orchestration platform. How can it enhance ETL pipeline\n",
    "deployment and management? Explain the concept of containerization and its benefits in\n",
    "this context.\n",
    "6. In the context of ETL pipelines, what are some common data transformation challenges\n",
    "you might encounter? Provide examples of transformations you might need to perform on\n",
    "the extracted data before loading it into the destination.\n",
    "\n",
    "**Part 3: Scalability and Monitoring**\n",
    "\n",
    "7. Scalability is crucial for handling large volumes of data. How would you design your ETL\n",
    "pipeline to handle an increasing amount of data over time? Discuss any relevant techniques\n",
    "or tools you would employ.\n",
    "8. Monitoring and logging are essential for maintaining the health and performance of ETL\n",
    "pipelines. What strategies would you implement to monitor the various components of your\n",
    "pipeline, and how would you handle error scenarios?\n",
    "\n",
    "**Part 4: Best Practices and Security**\n",
    "\n",
    "9. Data security is paramount when dealing with sensitive user information. Describe the\n",
    "security measures you would implement to ensure the privacy and integrity of the data\n",
    "flowing through your ETL pipeline.\n",
    "10. What are some best practices for documenting your ETL pipeline? Explain the\n",
    "importance of clear documentation and how it can benefit both your team and other\n",
    "stakeholders.\n",
    "\n",
    "**Part 5: Practical Scenario**\n",
    "\n",
    "11. Imagine you are tasked with building an ETL pipeline to consolidate data from Facebook\n",
    "Ads, Google Ads, RDS, and CleverTap into a data warehouse. Outline the high-level\n",
    "architecture of your solution, including the role of Airflow and Kubernetes, the data flow, and\n",
    "the tools you would use for data transformation and loading.\n",
    "\n",
    "**Submission Guidelines:**\n",
    "- Provide detailed explanations for each question to showcase your understanding.\n",
    "- Feel free to use diagrams, code snippets, or pseudocode to illustrate your points.\n",
    "- Your solutions will be assessed based on technical accuracy, depth of understanding, and\n",
    "practical feasibility.\n",
    "\n",
    "**Note:** This assessment is designed to gauge your expertise in building ETL pipelines\n",
    "using specific tools and technologies. It is recommended to take your time and provide\n",
    "thoughtful responses to demonstrate your skills effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
